# 2025-01-14 19:11:16 # 
# 2025-01-14 19:11:16 # ====================================================================================================
# 2025-01-14 19:11:16 # 
# 2025-01-14 19:11:16 # [INFO]  Configuration:
# 2025-01-14 19:11:16 # 	 -> train_hr_folder       : data/DIV2K_train_HR
# 2025-01-14 19:11:16 # 	 -> valid_hr_folder       : data/DIV2K_valid_HR
# 2025-01-14 19:11:16 # 	 -> train_lr_folder       : data/DIV2K_train_LR
# 2025-01-14 19:11:16 # 	 -> valid_lr_folder       : data/DIV2K_valid_LR
# 2025-01-14 19:11:16 # 	 -> train_teacher_folder  : data/DIV2K_train_teacher
# 2025-01-14 19:11:16 # 	 -> valid_teacher_folder  : data/DIV2K_valid_teacher
# 2025-01-14 19:11:16 # 	 -> overwrite_teacher_data: False
# 2025-01-14 19:11:16 # 	 -> model_id              : stabilityai/stable-diffusion-x4-upscaler
# 2025-01-14 19:11:16 # 	 -> teacher_prompt        : a photo
# 2025-01-14 19:11:16 # 	 -> num_inference_steps   : 5
# 2025-01-14 19:11:16 # 	 -> guidance_scale        : 0.0
# 2025-01-14 19:11:16 # 	 -> batch_size            : 1
# 2025-01-14 19:11:16 # 	 -> epochs                : 500
# 2025-01-14 19:11:16 # 	 -> learning_rate         : 0.0001
# 2025-01-14 19:11:16 # 	 -> weight_decay          : 0.0
# 2025-01-14 19:11:16 # 	 -> up_factor             : 4
# 2025-01-14 19:11:16 # 	 -> device                : cuda
# 2025-01-14 19:11:16 # 	 -> exp_name              : exp
# 2025-01-14 19:11:16 # 	 -> log_path              : log
# 2025-01-14 19:11:16 # 	 -> optimizer             : Adam
# 2025-01-14 19:11:16 # 	 -> scheduler             : None
# 2025-01-14 19:11:16 # 	 -> warmup_ratio          : 0.1
# 2025-01-14 19:11:16 # 	 -> warmup_steps          : 0
# 2025-01-14 19:11:16 # 	 -> clip_max_norm         : 1.0
# 2025-01-14 19:11:16 # 	 -> seed                  : 1998
# 2025-01-14 19:11:16 # 
# 2025-01-14 19:11:16 # ====================================================================================================
# 2025-01-14 19:11:16 # 
# 2025-01-14 19:11:16 # [Stage 1]  Preparing Low-Resolution Images
# 2025-01-14 19:11:16 # [Teacher]  Generating LR from training HR: data/DIV2K_train_HR, found 800 images.
# 2025-01-14 19:11:16 # [Teacher]  Generating LR from validation HR: data/DIV2K_valid_HR, found 100 images.
# 2025-01-14 19:11:16 # 
# 2025-01-14 19:11:16 # ====================================================================================================
# 2025-01-14 19:11:16 # 
# 2025-01-14 19:11:16 # [Stage 2]  Generating Teacher Outputs
# 2025-01-14 19:11:16 # [Teacher]  All teacher outputs already exist, and overwrite_teacher_data=False. Skipping upscaling.
# 2025-01-14 19:11:16 # 
# 2025-01-14 19:11:16 # ====================================================================================================
# 2025-01-14 19:11:16 # 
# 2025-01-14 19:11:16 # [Stage 3]  Student Training (Knowledge Distillation)
# 2025-01-14 19:11:16 # [Trainer] Starting knowledge distillation training...
# 2025-01-14 19:11:48 # Epoch 1/500 -> Train Loss: 0.0935 | Valid Loss: 0.0764 | Grad Norm: 0.192369 | LR: 0.000100
# 2025-01-14 19:12:19 # Epoch 2/500 -> Train Loss: 0.0656 | Valid Loss: 0.0712 | Grad Norm: 2.251633 | LR: 0.000100
# 2025-01-14 19:12:50 # Epoch 3/500 -> Train Loss: 0.0625 | Valid Loss: 0.0706 | Grad Norm: 0.552881 | LR: 0.000100
# 2025-01-14 19:13:20 # Epoch 4/500 -> Train Loss: 0.0605 | Valid Loss: 0.0689 | Grad Norm: 1.156766 | LR: 0.000100
# 2025-01-14 19:13:51 # Epoch 5/500 -> Train Loss: 0.0592 | Valid Loss: 0.0668 | Grad Norm: 0.648881 | LR: 0.000100
# 2025-01-14 19:14:22 # Epoch 6/500 -> Train Loss: 0.0586 | Valid Loss: 0.0672 | Grad Norm: 0.245513 | LR: 0.000100
# 2025-01-14 19:14:53 # Epoch 7/500 -> Train Loss: 0.0581 | Valid Loss: 0.0744 | Grad Norm: 0.553567 | LR: 0.000100
# 2025-01-14 19:15:23 # Epoch 8/500 -> Train Loss: 0.0574 | Valid Loss: 0.0684 | Grad Norm: 1.526489 | LR: 0.000100
# 2025-01-14 19:15:53 # Epoch 9/500 -> Train Loss: 0.0570 | Valid Loss: 0.0648 | Grad Norm: 1.078837 | LR: 0.000100
# 2025-01-14 19:16:24 # Epoch 10/500 -> Train Loss: 0.0568 | Valid Loss: 0.0647 | Grad Norm: 0.165614 | LR: 0.000100
# 2025-01-14 19:16:54 # Epoch 11/500 -> Train Loss: 0.0568 | Valid Loss: 0.0646 | Grad Norm: 1.015802 | LR: 0.000100
# 2025-01-14 19:17:25 # Epoch 12/500 -> Train Loss: 0.0561 | Valid Loss: 0.0644 | Grad Norm: 0.374706 | LR: 0.000100
# 2025-01-14 19:17:56 # Epoch 13/500 -> Train Loss: 0.0561 | Valid Loss: 0.0644 | Grad Norm: 1.040751 | LR: 0.000100
# 2025-01-14 19:18:26 # Epoch 14/500 -> Train Loss: 0.0560 | Valid Loss: 0.0644 | Grad Norm: 0.782275 | LR: 0.000100
# 2025-01-14 19:18:57 # Epoch 15/500 -> Train Loss: 0.0557 | Valid Loss: 0.0634 | Grad Norm: 0.763885 | LR: 0.000100
# 2025-01-14 19:19:28 # Epoch 16/500 -> Train Loss: 0.0556 | Valid Loss: 0.0639 | Grad Norm: 0.210334 | LR: 0.000100
# 2025-01-14 19:19:58 # Epoch 17/500 -> Train Loss: 0.0550 | Valid Loss: 0.0661 | Grad Norm: 1.039179 | LR: 0.000100
# 2025-01-14 19:20:29 # Epoch 18/500 -> Train Loss: 0.0557 | Valid Loss: 0.0638 | Grad Norm: 0.463301 | LR: 0.000100
# 2025-01-14 19:20:59 # Epoch 19/500 -> Train Loss: 0.0550 | Valid Loss: 0.0632 | Grad Norm: 0.423252 | LR: 0.000100
# 2025-01-14 19:21:30 # Epoch 20/500 -> Train Loss: 0.0548 | Valid Loss: 0.0642 | Grad Norm: 0.791607 | LR: 0.000100
# 2025-01-14 19:22:00 # Epoch 21/500 -> Train Loss: 0.0545 | Valid Loss: 0.0648 | Grad Norm: 0.443517 | LR: 0.000100
# 2025-01-14 19:22:31 # Epoch 22/500 -> Train Loss: 0.0550 | Valid Loss: 0.0642 | Grad Norm: 0.182192 | LR: 0.000100
# 2025-01-14 19:23:01 # Epoch 23/500 -> Train Loss: 0.0546 | Valid Loss: 0.0634 | Grad Norm: 0.925921 | LR: 0.000100
# 2025-01-14 19:23:32 # Epoch 24/500 -> Train Loss: 0.0545 | Valid Loss: 0.0649 | Grad Norm: 0.331138 | LR: 0.000100
# 2025-01-14 19:24:02 # Epoch 25/500 -> Train Loss: 0.0544 | Valid Loss: 0.0657 | Grad Norm: 0.484892 | LR: 0.000100
# 2025-01-14 19:24:33 # Epoch 26/500 -> Train Loss: 0.0543 | Valid Loss: 0.0640 | Grad Norm: 1.310332 | LR: 0.000100
# 2025-01-14 19:25:04 # Epoch 27/500 -> Train Loss: 0.0545 | Valid Loss: 0.0655 | Grad Norm: 0.486274 | LR: 0.000100
# 2025-01-14 19:25:35 # Epoch 28/500 -> Train Loss: 0.0540 | Valid Loss: 0.0631 | Grad Norm: 0.244408 | LR: 0.000100
# 2025-01-14 19:26:05 # Epoch 29/500 -> Train Loss: 0.0544 | Valid Loss: 0.0643 | Grad Norm: 0.583940 | LR: 0.000100
# 2025-01-14 19:26:36 # Epoch 30/500 -> Train Loss: 0.0537 | Valid Loss: 0.0633 | Grad Norm: 0.274384 | LR: 0.000100
# 2025-01-14 19:27:07 # Epoch 31/500 -> Train Loss: 0.0539 | Valid Loss: 0.0624 | Grad Norm: 0.200892 | LR: 0.000100
# 2025-01-14 19:27:38 # Epoch 32/500 -> Train Loss: 0.0543 | Valid Loss: 0.0638 | Grad Norm: 1.006229 | LR: 0.000100
# 2025-01-14 19:28:09 # Epoch 33/500 -> Train Loss: 0.0538 | Valid Loss: 0.0642 | Grad Norm: 0.301210 | LR: 0.000100
# 2025-01-14 19:28:40 # Epoch 34/500 -> Train Loss: 0.0537 | Valid Loss: 0.0651 | Grad Norm: 0.175053 | LR: 0.000100
# 2025-01-14 19:29:11 # Epoch 35/500 -> Train Loss: 0.0533 | Valid Loss: 0.0640 | Grad Norm: 1.092488 | LR: 0.000100
# 2025-01-14 19:29:41 # Epoch 36/500 -> Train Loss: 0.0535 | Valid Loss: 0.0634 | Grad Norm: 0.354950 | LR: 0.000100
# 2025-01-14 19:30:12 # Epoch 37/500 -> Train Loss: 0.0537 | Valid Loss: 0.0652 | Grad Norm: 0.694422 | LR: 0.000100
# 2025-01-14 19:30:43 # Epoch 38/500 -> Train Loss: 0.0532 | Valid Loss: 0.0628 | Grad Norm: 1.138878 | LR: 0.000100
# 2025-01-14 19:31:14 # Epoch 39/500 -> Train Loss: 0.0533 | Valid Loss: 0.0631 | Grad Norm: 1.224832 | LR: 0.000100
# 2025-01-14 19:31:45 # Epoch 40/500 -> Train Loss: 0.0532 | Valid Loss: 0.0631 | Grad Norm: 0.312161 | LR: 0.000100
# 2025-01-14 19:32:16 # Epoch 41/500 -> Train Loss: 0.0532 | Valid Loss: 0.0621 | Grad Norm: 0.285115 | LR: 0.000100
# 2025-01-14 19:32:47 # Epoch 42/500 -> Train Loss: 0.0532 | Valid Loss: 0.0627 | Grad Norm: 0.069200 | LR: 0.000100
# 2025-01-14 19:33:17 # Epoch 43/500 -> Train Loss: 0.0531 | Valid Loss: 0.0628 | Grad Norm: 0.174131 | LR: 0.000100
# 2025-01-14 19:33:48 # Epoch 44/500 -> Train Loss: 0.0527 | Valid Loss: 0.0630 | Grad Norm: 1.949070 | LR: 0.000100
# 2025-01-14 19:34:19 # Epoch 45/500 -> Train Loss: 0.0528 | Valid Loss: 0.0625 | Grad Norm: 0.739203 | LR: 0.000100
# 2025-01-14 19:34:50 # Epoch 46/500 -> Train Loss: 0.0527 | Valid Loss: 0.0643 | Grad Norm: 1.085560 | LR: 0.000100
# 2025-01-14 19:35:21 # Epoch 47/500 -> Train Loss: 0.0529 | Valid Loss: 0.0632 | Grad Norm: 0.659731 | LR: 0.000100
# 2025-01-14 19:35:52 # Epoch 48/500 -> Train Loss: 0.0525 | Valid Loss: 0.0623 | Grad Norm: 0.223710 | LR: 0.000100
# 2025-01-14 19:36:23 # Epoch 49/500 -> Train Loss: 0.0527 | Valid Loss: 0.0618 | Grad Norm: 0.672749 | LR: 0.000100
# 2025-01-14 19:36:54 # Epoch 50/500 -> Train Loss: 0.0528 | Valid Loss: 0.0633 | Grad Norm: 0.396566 | LR: 0.000100
# 2025-01-14 19:37:25 # Epoch 51/500 -> Train Loss: 0.0526 | Valid Loss: 0.0622 | Grad Norm: 0.471540 | LR: 0.000100
# 2025-01-14 19:37:55 # Epoch 52/500 -> Train Loss: 0.0525 | Valid Loss: 0.0631 | Grad Norm: 0.389474 | LR: 0.000100
# 2025-01-14 19:38:26 # Epoch 53/500 -> Train Loss: 0.0526 | Valid Loss: 0.0620 | Grad Norm: 0.433360 | LR: 0.000100
# 2025-01-14 19:38:57 # Epoch 54/500 -> Train Loss: 0.0526 | Valid Loss: 0.0642 | Grad Norm: 0.967835 | LR: 0.000100
# 2025-01-14 19:39:28 # Epoch 55/500 -> Train Loss: 0.0524 | Valid Loss: 0.0621 | Grad Norm: 0.222482 | LR: 0.000100
# 2025-01-14 19:39:58 # Epoch 56/500 -> Train Loss: 0.0523 | Valid Loss: 0.0618 | Grad Norm: 0.089236 | LR: 0.000100
# 2025-01-14 19:40:29 # Epoch 57/500 -> Train Loss: 0.0522 | Valid Loss: 0.0614 | Grad Norm: 0.312798 | LR: 0.000100
# 2025-01-14 19:41:00 # Epoch 58/500 -> Train Loss: 0.0522 | Valid Loss: 0.0623 | Grad Norm: 0.780584 | LR: 0.000100
# 2025-01-14 19:41:31 # Epoch 59/500 -> Train Loss: 0.0522 | Valid Loss: 0.0619 | Grad Norm: 0.637732 | LR: 0.000100
# 2025-01-14 19:42:02 # Epoch 60/500 -> Train Loss: 0.0522 | Valid Loss: 0.0622 | Grad Norm: 0.248429 | LR: 0.000100
# 2025-01-14 19:42:33 # Epoch 61/500 -> Train Loss: 0.0521 | Valid Loss: 0.0626 | Grad Norm: 1.318522 | LR: 0.000100
# 2025-01-14 19:43:04 # Epoch 62/500 -> Train Loss: 0.0522 | Valid Loss: 0.0621 | Grad Norm: 0.237818 | LR: 0.000100
# 2025-01-14 19:43:35 # Epoch 63/500 -> Train Loss: 0.0520 | Valid Loss: 0.0615 | Grad Norm: 0.961564 | LR: 0.000100
# 2025-01-14 19:44:05 # Epoch 64/500 -> Train Loss: 0.0520 | Valid Loss: 0.0617 | Grad Norm: 0.242479 | LR: 0.000100
# 2025-01-14 19:44:36 # Epoch 65/500 -> Train Loss: 0.0518 | Valid Loss: 0.0614 | Grad Norm: 0.301162 | LR: 0.000100
# 2025-01-14 19:45:07 # Epoch 66/500 -> Train Loss: 0.0519 | Valid Loss: 0.0618 | Grad Norm: 0.442428 | LR: 0.000100
# 2025-01-14 19:45:38 # Epoch 67/500 -> Train Loss: 0.0519 | Valid Loss: 0.0616 | Grad Norm: 0.661938 | LR: 0.000100
# 2025-01-14 19:46:08 # Epoch 68/500 -> Train Loss: 0.0518 | Valid Loss: 0.0615 | Grad Norm: 0.323601 | LR: 0.000100
# 2025-01-14 19:46:39 # Epoch 69/500 -> Train Loss: 0.0519 | Valid Loss: 0.0621 | Grad Norm: 0.320895 | LR: 0.000100
# 2025-01-14 19:47:10 # Epoch 70/500 -> Train Loss: 0.0521 | Valid Loss: 0.0620 | Grad Norm: 0.164444 | LR: 0.000100
# 2025-01-14 19:47:41 # Epoch 71/500 -> Train Loss: 0.0517 | Valid Loss: 0.0617 | Grad Norm: 0.201228 | LR: 0.000100
# 2025-01-14 19:48:12 # Epoch 72/500 -> Train Loss: 0.0518 | Valid Loss: 0.0626 | Grad Norm: 0.413932 | LR: 0.000100
# 2025-01-14 19:48:43 # Epoch 73/500 -> Train Loss: 0.0520 | Valid Loss: 0.0623 | Grad Norm: 0.740187 | LR: 0.000100
# 2025-01-14 19:49:14 # Epoch 74/500 -> Train Loss: 0.0516 | Valid Loss: 0.0612 | Grad Norm: 0.148242 | LR: 0.000100
# 2025-01-14 19:49:44 # Epoch 75/500 -> Train Loss: 0.0515 | Valid Loss: 0.0619 | Grad Norm: 0.106197 | LR: 0.000100
# 2025-01-14 19:50:15 # Epoch 76/500 -> Train Loss: 0.0518 | Valid Loss: 0.0614 | Grad Norm: 0.492281 | LR: 0.000100
# 2025-01-14 19:50:46 # Epoch 77/500 -> Train Loss: 0.0515 | Valid Loss: 0.0635 | Grad Norm: 0.158377 | LR: 0.000100
# 2025-01-14 19:51:17 # Epoch 78/500 -> Train Loss: 0.0516 | Valid Loss: 0.0608 | Grad Norm: 0.150895 | LR: 0.000100
# 2025-01-14 19:51:47 # Epoch 79/500 -> Train Loss: 0.0517 | Valid Loss: 0.0625 | Grad Norm: 0.497391 | LR: 0.000100
# 2025-01-14 19:52:18 # Epoch 80/500 -> Train Loss: 0.0517 | Valid Loss: 0.0617 | Grad Norm: 0.250258 | LR: 0.000100
# 2025-01-14 19:52:49 # Epoch 81/500 -> Train Loss: 0.0513 | Valid Loss: 0.0615 | Grad Norm: 0.504572 | LR: 0.000100
# 2025-01-14 19:53:20 # Epoch 82/500 -> Train Loss: 0.0516 | Valid Loss: 0.0609 | Grad Norm: 0.360796 | LR: 0.000100
# 2025-01-14 19:53:50 # Epoch 83/500 -> Train Loss: 0.0513 | Valid Loss: 0.0629 | Grad Norm: 0.861731 | LR: 0.000100
# 2025-01-14 19:54:21 # Epoch 84/500 -> Train Loss: 0.0515 | Valid Loss: 0.0611 | Grad Norm: 0.345393 | LR: 0.000100
# 2025-01-14 19:54:51 # Epoch 85/500 -> Train Loss: 0.0514 | Valid Loss: 0.0610 | Grad Norm: 0.410930 | LR: 0.000100
# 2025-01-14 19:55:22 # Epoch 86/500 -> Train Loss: 0.0514 | Valid Loss: 0.0613 | Grad Norm: 0.980503 | LR: 0.000100
# 2025-01-14 19:55:52 # Epoch 87/500 -> Train Loss: 0.0513 | Valid Loss: 0.0625 | Grad Norm: 0.356867 | LR: 0.000100
# 2025-01-14 19:56:23 # Epoch 88/500 -> Train Loss: 0.0514 | Valid Loss: 0.0615 | Grad Norm: 0.812727 | LR: 0.000100
# 2025-01-14 19:56:54 # Epoch 89/500 -> Train Loss: 0.0514 | Valid Loss: 0.0610 | Grad Norm: 1.686614 | LR: 0.000100
# 2025-01-14 19:57:24 # Epoch 90/500 -> Train Loss: 0.0512 | Valid Loss: 0.0613 | Grad Norm: 0.141162 | LR: 0.000100
# 2025-01-14 19:57:55 # Epoch 91/500 -> Train Loss: 0.0513 | Valid Loss: 0.0607 | Grad Norm: 0.767566 | LR: 0.000100
